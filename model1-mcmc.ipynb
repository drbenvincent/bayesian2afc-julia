{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 - MCMC with Turing.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below _should_ work when executed. I am having difficulties in getting the Turing.jl package to work with the Jupyter notebook, but it runs just fine for me when run in a Julia console session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Turing\n",
    "using Distributions, StatsFuns\n",
    "using Mamba: describe, plot\n",
    "\n",
    "# Create data ---------------------------------------------\n",
    "data = Dict()\n",
    "data[\"T\"] = 100\n",
    "data[\"k\"] = [50, 51, 57, 55, 63, 62, 82, 94, 99, 100]\n",
    "data[\"Δμ\"] = [0.0100, 0.0215, 0.0464, 0.1000, 0.2154, 0.4642, 1.0000, 2.1544, 4.6416, 10.0000]\n",
    "\n",
    "\n",
    "# Conduct inference ---------------------------------------\n",
    "# First we are going to define a model. This is the way of doing it for the Turing package. \n",
    "# I managed to convert this over from the JAGS specification (see the original Matlab repo) \n",
    "# with not so much trouble. It's pretty nice in fact.\n",
    "@model model1(k, Δμ, T) = begin\n",
    "    Φ(x) = normcdf(0, 1, x)\n",
    "    PC(Δμ, σ²) = Φ(Δμ/sqrt(2σ²))\n",
    "    σ² ~ Uniform(0,100)\n",
    "    for c = 1:length(k)\n",
    "        k[c] ~ Binomial(T, PC(Δμ[c],σ²))\n",
    "    end\n",
    "    return σ²\n",
    "end\n",
    "\n",
    "# do sampling. Note that there are a variety of sampling algorithms we can specify\n",
    "n_samples = 300\n",
    "samples = sample( model1(data[\"k\"], data[\"Δμ\"], data[\"T\"]), PG(50,n_samples))\n",
    "σ²posterior = samples[:σ²]\n",
    "\n",
    "# TODO: plot histogram of σ²posterior\n",
    "\n",
    "# Posterior prediction ------------------------------------\n",
    "\n",
    "function posterior_prediction(T::Int64, Δμ_list, σ²samples)\n",
    "    Φ(x) = normcdf(0, 1, x)\n",
    "    PC(Δμ, σ²) = Φ(Δμ/sqrt(2σ²))\n",
    "    k_pred = zeros(n_samples, length(data[\"Δμ\"]))\n",
    "    k_pred = [rand(Binomial(T, PC(Δμ,σ²))) for Δμ in data[\"Δμ_list\"], σ² in σ²samples]\n",
    "end\n",
    "\n",
    "k_pred = posterior_prediction(data[\"T\"], data[\"Δμ\"], σ²posterior)\n",
    "# so now we have posterior predictive samples in k_pred.\n",
    "# Each row corresponds to an mcmc sample, each column corresponds to a value of Δμ\n",
    "\n",
    "\n",
    "# TODO: do summary stats or plotting of posterior predictions, and plot the actual data so we\n",
    "# can do model critique visually\n",
    "\n",
    "# TODO: can also do out of sample prediction simply by providing more values for Δμ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "Vincent, B. T. (2015). A tutorial on Bayesian models of perception. Journal of Mathematical Psychology, 66, 103–114. http://doi.org/10.1016/j.jmp.2015.02.001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
